{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6e4d225",
   "metadata": {},
   "source": [
    "# Chrono24 Watch Scraper with Selenium\n",
    "\n",
    "This notebook contains a complete Selenium-based web scraper for extracting watch data from Chrono24.com. The scraper bypasses anti-bot protection by using real Chrome browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d43f27eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import logging\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "21c078fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict\n",
    "\n",
    "@dataclass\n",
    "class WatchItem:\n",
    "    watch_id: str = \"\"\n",
    "    watch_title: str = \"\"\n",
    "    watch_price: str = \"\"\n",
    "    watch_details: Dict = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.watch_details is None:\n",
    "            self.watch_details = {}\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        return {\n",
    "            \"watch_id\": self.watch_id,\n",
    "            \"watch_title\": self.watch_title,\n",
    "            \"watch_price\": self.watch_price,\n",
    "            \"watch_details\": self.watch_details\n",
    "        }\n",
    "    \n",
    "    def to_flat_dict(self) -> Dict:\n",
    "        flat_dict = {\n",
    "            \"watch_id\": self.watch_id,\n",
    "            \"watch_title\": self.watch_title,\n",
    "            \"watch_price\": self.watch_price\n",
    "        }\n",
    "        \n",
    "        if self.watch_details:\n",
    "            for section, details in self.watch_details.items():\n",
    "                if isinstance(details, dict):\n",
    "                    for key, value in details.items():\n",
    "                        column_name = f\"{section}_{key}\".replace(\" \", \"_\").replace(\"/\", \"_\")\n",
    "                        flat_dict[column_name] = value\n",
    "        \n",
    "        return flat_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "29f2456d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeleniumWebDriver:\n",
    "    def __init__(self, headless: bool = True):\n",
    "        self.driver = None\n",
    "        self.headless = headless\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "    def setup_driver(self):\n",
    "        chrome_options = Options()\n",
    "        \n",
    "        chrome_options.add_argument(\"--no-sandbox\")\n",
    "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "        chrome_options.add_argument(\"--disable-extensions\")\n",
    "        chrome_options.add_argument(\"--disable-plugins\")\n",
    "        chrome_options.add_argument(\"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\")\n",
    "        \n",
    "        if self.headless:\n",
    "            chrome_options.add_argument(\"--headless\")\n",
    "        \n",
    "        chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "        chrome_options.add_experimental_option('useAutomationExtension', False)\n",
    "        \n",
    "        chrome_options.add_experimental_option(\"prefs\", {\n",
    "            \"profile.default_content_setting_values.notifications\": 2\n",
    "        })\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        self.driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "        \n",
    "        self.driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "        \n",
    "        return self.driver\n",
    "    \n",
    "    def close_driver(self):\n",
    "        if self.driver:\n",
    "            self.driver.quit()\n",
    "\n",
    "web_driver = SeleniumWebDriver(headless=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "196a3650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chrono24 scraper started\n"
     ]
    }
   ],
   "source": [
    "class Chrono24Scraper:\n",
    "    def __init__(self, web_driver_manager: SeleniumWebDriver):\n",
    "        self.web_driver_manager = web_driver_manager\n",
    "        self.driver = None\n",
    "        self.base_url = \"https://www.chrono24.com\"\n",
    "        self.scraped_items = []\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.pages_scraped = 0\n",
    "        self.max_pages = None\n",
    "        \n",
    "    def start_scraping(self, start_url: str, max_pages: int = None):\n",
    "        self.max_pages = max_pages\n",
    "        self.pages_scraped = 0\n",
    "        self.driver = self.web_driver_manager.setup_driver()\n",
    "        print(f\"Starting scraper with max_pages: {max_pages if max_pages else 'unlimited'}\")\n",
    "        \n",
    "        self.scrape_pages_by_url_construction(start_url)\n",
    "        \n",
    "    def scrape_pages_by_url_construction(self, base_url: str):\n",
    "        page_num = 1\n",
    "        \n",
    "        while True:\n",
    "            if self.max_pages and page_num > self.max_pages:\n",
    "                print(f\"Reached maximum pages limit: {self.max_pages}\")\n",
    "                break\n",
    "                \n",
    "            if page_num == 1:\n",
    "                current_url = base_url\n",
    "            else:\n",
    "                if \"index.htm\" in base_url:\n",
    "                    current_url = base_url.replace(\"index.htm\", f\"index-{page_num}.htm\")\n",
    "                else:\n",
    "                    current_url = f\"{base_url.rstrip('/')}/index-{page_num}.htm\"\n",
    "            \n",
    "            print(f\"Loading page {page_num}: {current_url}\")\n",
    "            \n",
    "            if not self.scrape_single_page(current_url, page_num):\n",
    "                print(f\"Page {page_num} not found or empty, stopping pagination\")\n",
    "                break\n",
    "                \n",
    "            page_num += 1\n",
    "            time.sleep(3)\n",
    "    \n",
    "    def scrape_single_page(self, url: str, page_num: int):\n",
    "        try:\n",
    "            self.driver.get(url)\n",
    "            time.sleep(3)\n",
    "            \n",
    "            try:\n",
    "                WebDriverWait(self.driver, 10).until(\n",
    "                    EC.presence_of_element_located((By.CSS_SELECTOR, \".article-item-container\"))\n",
    "                )\n",
    "            except TimeoutException:\n",
    "                print(f\"No watch listings found on page {page_num}\")\n",
    "                return False\n",
    "            \n",
    "            soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "            \n",
    "            watch_links = soup.select('.article-item-container > a')\n",
    "            \n",
    "            if not watch_links:\n",
    "                print(f\"No watch links found on page {page_num}\")\n",
    "                return False\n",
    "                \n",
    "            self.pages_scraped += 1\n",
    "            print(f\"Found {len(watch_links)} watch listings on page {page_num}\")\n",
    "            \n",
    "            for i, link in enumerate(watch_links, 1):\n",
    "                href = link.get('href')\n",
    "                if href:\n",
    "                    watch_url = self.base_url + href\n",
    "                    print(f\"Processing watch {i}/{len(watch_links)} on page {page_num}\")\n",
    "                    self.scrape_watch_detail(watch_url)\n",
    "                    time.sleep(1)\n",
    "            \n",
    "            print(f\"Completed page {page_num}. Total watches so far: {len(self.scraped_items)}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping page {page_num}: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def scrape_watch_detail(self, url: str):\n",
    "        try:\n",
    "            self.driver.get(url)\n",
    "            time.sleep(2)\n",
    "            \n",
    "            soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "            \n",
    "            watch_item = WatchItem()\n",
    "            \n",
    "            watch_id_element = soup.select_one(\".wt-share-offer\")\n",
    "            if watch_id_element:\n",
    "                watch_item.watch_id = watch_id_element.get('data-watch-id', '')\n",
    "            \n",
    "            title_element = soup.select_one(\"h1.h3\")\n",
    "            if title_element:\n",
    "                watch_item.watch_title = title_element.get_text(strip=True)\n",
    "            \n",
    "            price_element = soup.select_one(\".js-price-shipping-country\")\n",
    "            if price_element:\n",
    "                watch_item.watch_price = price_element.get_text(strip=True)\n",
    "            \n",
    "            watch_item.watch_details = self.extract_watch_details(soup)\n",
    "            \n",
    "            self.scraped_items.append(watch_item)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping watch detail: {e}\")\n",
    "    \n",
    "    def extract_watch_details(self, soup: BeautifulSoup) -> Dict:\n",
    "        watch_data = {}\n",
    "        section_name = None\n",
    "        \n",
    "        try:\n",
    "            table = soup.select_one('table')\n",
    "            if not table:\n",
    "                return watch_data\n",
    "            \n",
    "            rows = table.select('tr')\n",
    "            for row in rows:\n",
    "                section_header = row.select_one('td[colspan] h3')\n",
    "                if section_header:\n",
    "                    section_name = section_header.get_text(strip=True)\n",
    "                    watch_data[section_name] = {}\n",
    "                elif section_name:\n",
    "                    cells = row.select('td')\n",
    "                    if len(cells) >= 2:\n",
    "                        key_element = cells[0].select_one('strong')\n",
    "                        value_element = cells[1]\n",
    "                        \n",
    "                        if key_element and value_element:\n",
    "                            key = key_element.get_text(strip=True)\n",
    "                            value = value_element.get_text(strip=True)\n",
    "                            if key and value:\n",
    "                                watch_data[section_name][key] = value\n",
    "                                \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting watch details: {e}\")\n",
    "            \n",
    "        return watch_data\n",
    "    \n",
    "    def save_to_json(self, filename: str = \"chrono24_watches.json\"):\n",
    "        try:\n",
    "            data = [item.to_dict() for item in self.scraped_items]\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "            return filename\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving to JSON: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def save_to_csv(self, filename: str = \"chrono24_watches.csv\"):\n",
    "        try:\n",
    "            flat_data = [item.to_flat_dict() for item in self.scraped_items]\n",
    "            df = pd.DataFrame(flat_data)\n",
    "            df.to_csv(filename, index=False, encoding='utf-8')\n",
    "            return filename\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving to CSV: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def get_progress_info(self):\n",
    "        return {\n",
    "            \"pages_scraped\": self.pages_scraped,\n",
    "            \"max_pages\": self.max_pages,\n",
    "            \"total_watches\": len(self.scraped_items),\n",
    "            \"avg_watches_per_page\": len(self.scraped_items) / self.pages_scraped if self.pages_scraped > 0 else 0\n",
    "        }\n",
    "    \n",
    "    def close(self):\n",
    "        self.web_driver_manager.close_driver()\n",
    "\n",
    "scraper = Chrono24Scraper(web_driver)\n",
    "print(\"Chrono24 scraper started\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f05239",
   "metadata": {},
   "source": [
    "## Configuration and Usage\n",
    "\n",
    "Now you can run the scraper with different configurations. The scraper will:\n",
    "\n",
    "1. **Load the target page** using Selenium Chrome\n",
    "2. **Extract watch listings** from the category page\n",
    "3. **Follow each watch link** to get detailed specifications\n",
    "4. **Parse structured data** from the specifications table\n",
    "5. **Handle pagination** automatically\n",
    "6. **Save results** to CSV and JSON file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "25a260d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting scraper for 3 pages\n",
      "Target URL: https://www.chrono24.com/rolex/index.htm\n",
      "Starting scraper with max_pages: 3\n",
      "Loading page 1: https://www.chrono24.com/rolex/index.htm\n",
      "Starting scraper with max_pages: 3\n",
      "Loading page 1: https://www.chrono24.com/rolex/index.htm\n",
      "Found 60 watch listings on page 1\n",
      "Processing watch 1/60 on page 1\n",
      "Found 60 watch listings on page 1\n",
      "Processing watch 1/60 on page 1\n",
      "Processing watch 2/60 on page 1\n",
      "Processing watch 2/60 on page 1\n",
      "Processing watch 3/60 on page 1\n",
      "Processing watch 3/60 on page 1\n",
      "Processing watch 4/60 on page 1\n",
      "Processing watch 4/60 on page 1\n",
      "Processing watch 5/60 on page 1\n",
      "Processing watch 5/60 on page 1\n",
      "Processing watch 6/60 on page 1\n",
      "Processing watch 6/60 on page 1\n",
      "Processing watch 7/60 on page 1\n",
      "Processing watch 7/60 on page 1\n",
      "Processing watch 8/60 on page 1\n",
      "Processing watch 8/60 on page 1\n",
      "Processing watch 9/60 on page 1\n",
      "Processing watch 9/60 on page 1\n",
      "Processing watch 10/60 on page 1\n",
      "Processing watch 10/60 on page 1\n",
      "Processing watch 11/60 on page 1\n",
      "Processing watch 11/60 on page 1\n",
      "Processing watch 12/60 on page 1\n",
      "Processing watch 12/60 on page 1\n",
      "Processing watch 13/60 on page 1\n",
      "Processing watch 13/60 on page 1\n",
      "Processing watch 14/60 on page 1\n",
      "Processing watch 14/60 on page 1\n",
      "Processing watch 15/60 on page 1\n",
      "Processing watch 15/60 on page 1\n",
      "Processing watch 16/60 on page 1\n",
      "Processing watch 16/60 on page 1\n",
      "Processing watch 17/60 on page 1\n",
      "Processing watch 17/60 on page 1\n",
      "Processing watch 18/60 on page 1\n",
      "Processing watch 18/60 on page 1\n",
      "Processing watch 19/60 on page 1\n",
      "Processing watch 19/60 on page 1\n",
      "Processing watch 20/60 on page 1\n",
      "Processing watch 20/60 on page 1\n",
      "Processing watch 21/60 on page 1\n",
      "Processing watch 21/60 on page 1\n",
      "Processing watch 22/60 on page 1\n",
      "Processing watch 22/60 on page 1\n",
      "Processing watch 23/60 on page 1\n",
      "Processing watch 23/60 on page 1\n",
      "Processing watch 24/60 on page 1\n",
      "Processing watch 24/60 on page 1\n",
      "Processing watch 25/60 on page 1\n",
      "Processing watch 25/60 on page 1\n",
      "Processing watch 26/60 on page 1\n",
      "Processing watch 26/60 on page 1\n",
      "Processing watch 27/60 on page 1\n",
      "Processing watch 27/60 on page 1\n",
      "Processing watch 28/60 on page 1\n",
      "Processing watch 28/60 on page 1\n",
      "Processing watch 29/60 on page 1\n",
      "Processing watch 29/60 on page 1\n",
      "Processing watch 30/60 on page 1\n",
      "Processing watch 30/60 on page 1\n",
      "Processing watch 31/60 on page 1\n",
      "Processing watch 31/60 on page 1\n",
      "Processing watch 32/60 on page 1\n",
      "Processing watch 32/60 on page 1\n",
      "Processing watch 33/60 on page 1\n",
      "Processing watch 33/60 on page 1\n",
      "Processing watch 34/60 on page 1\n",
      "Processing watch 34/60 on page 1\n",
      "Processing watch 35/60 on page 1\n",
      "Processing watch 35/60 on page 1\n",
      "Processing watch 36/60 on page 1\n",
      "Processing watch 36/60 on page 1\n",
      "Processing watch 37/60 on page 1\n",
      "Processing watch 37/60 on page 1\n",
      "Processing watch 38/60 on page 1\n",
      "Processing watch 38/60 on page 1\n",
      "Processing watch 39/60 on page 1\n",
      "Processing watch 39/60 on page 1\n",
      "Processing watch 40/60 on page 1\n",
      "Processing watch 40/60 on page 1\n",
      "Processing watch 41/60 on page 1\n",
      "Processing watch 41/60 on page 1\n",
      "Processing watch 42/60 on page 1\n",
      "Processing watch 42/60 on page 1\n",
      "Processing watch 43/60 on page 1\n",
      "Processing watch 43/60 on page 1\n",
      "Processing watch 44/60 on page 1\n",
      "Processing watch 44/60 on page 1\n",
      "Processing watch 45/60 on page 1\n",
      "Processing watch 45/60 on page 1\n",
      "Processing watch 46/60 on page 1\n",
      "Processing watch 46/60 on page 1\n",
      "Processing watch 47/60 on page 1\n",
      "Processing watch 47/60 on page 1\n",
      "Processing watch 48/60 on page 1\n",
      "Processing watch 48/60 on page 1\n",
      "Processing watch 49/60 on page 1\n",
      "Processing watch 49/60 on page 1\n",
      "Processing watch 50/60 on page 1\n",
      "Processing watch 50/60 on page 1\n",
      "Processing watch 51/60 on page 1\n",
      "Processing watch 51/60 on page 1\n",
      "Processing watch 52/60 on page 1\n",
      "Processing watch 52/60 on page 1\n",
      "Processing watch 53/60 on page 1\n",
      "Processing watch 53/60 on page 1\n",
      "Processing watch 54/60 on page 1\n",
      "Processing watch 54/60 on page 1\n",
      "Processing watch 55/60 on page 1\n",
      "Processing watch 55/60 on page 1\n",
      "Processing watch 56/60 on page 1\n",
      "Processing watch 56/60 on page 1\n",
      "Processing watch 57/60 on page 1\n",
      "Processing watch 57/60 on page 1\n",
      "Processing watch 58/60 on page 1\n",
      "Processing watch 58/60 on page 1\n",
      "Processing watch 59/60 on page 1\n",
      "Processing watch 59/60 on page 1\n",
      "Processing watch 60/60 on page 1\n",
      "Processing watch 60/60 on page 1\n",
      "Completed page 1. Total watches so far: 60\n",
      "Completed page 1. Total watches so far: 60\n",
      "Loading page 2: https://www.chrono24.com/rolex/index-2.htm\n",
      "Loading page 2: https://www.chrono24.com/rolex/index-2.htm\n",
      "Found 60 watch listings on page 2\n",
      "Processing watch 1/60 on page 2\n",
      "Found 60 watch listings on page 2\n",
      "Processing watch 1/60 on page 2\n",
      "Processing watch 2/60 on page 2\n",
      "Processing watch 2/60 on page 2\n",
      "Processing watch 3/60 on page 2\n",
      "Processing watch 3/60 on page 2\n",
      "Processing watch 4/60 on page 2\n",
      "Processing watch 4/60 on page 2\n",
      "Processing watch 5/60 on page 2\n",
      "Processing watch 5/60 on page 2\n",
      "Processing watch 6/60 on page 2\n",
      "Processing watch 6/60 on page 2\n",
      "Processing watch 7/60 on page 2\n",
      "Processing watch 7/60 on page 2\n",
      "Processing watch 8/60 on page 2\n",
      "Processing watch 8/60 on page 2\n",
      "Processing watch 9/60 on page 2\n",
      "Processing watch 9/60 on page 2\n",
      "Processing watch 10/60 on page 2\n",
      "Processing watch 10/60 on page 2\n",
      "Processing watch 11/60 on page 2\n",
      "Processing watch 11/60 on page 2\n",
      "Processing watch 12/60 on page 2\n",
      "Processing watch 12/60 on page 2\n",
      "Processing watch 13/60 on page 2\n",
      "Processing watch 13/60 on page 2\n",
      "Processing watch 14/60 on page 2\n",
      "Processing watch 14/60 on page 2\n",
      "Processing watch 15/60 on page 2\n",
      "Processing watch 15/60 on page 2\n",
      "Processing watch 16/60 on page 2\n",
      "Processing watch 16/60 on page 2\n",
      "Processing watch 17/60 on page 2\n",
      "Processing watch 17/60 on page 2\n",
      "Processing watch 18/60 on page 2\n",
      "Processing watch 18/60 on page 2\n",
      "Processing watch 19/60 on page 2\n",
      "Processing watch 19/60 on page 2\n",
      "Processing watch 20/60 on page 2\n",
      "Processing watch 20/60 on page 2\n",
      "Processing watch 21/60 on page 2\n",
      "Processing watch 21/60 on page 2\n",
      "Processing watch 22/60 on page 2\n",
      "Processing watch 22/60 on page 2\n",
      "Processing watch 23/60 on page 2\n",
      "Processing watch 23/60 on page 2\n",
      "Processing watch 24/60 on page 2\n",
      "Processing watch 24/60 on page 2\n",
      "Processing watch 25/60 on page 2\n",
      "Processing watch 25/60 on page 2\n",
      "Processing watch 26/60 on page 2\n",
      "Processing watch 26/60 on page 2\n",
      "Processing watch 27/60 on page 2\n",
      "Processing watch 27/60 on page 2\n",
      "Processing watch 28/60 on page 2\n",
      "Processing watch 28/60 on page 2\n",
      "Processing watch 29/60 on page 2\n",
      "Processing watch 29/60 on page 2\n",
      "Processing watch 30/60 on page 2\n",
      "Processing watch 30/60 on page 2\n",
      "Processing watch 31/60 on page 2\n",
      "Processing watch 31/60 on page 2\n",
      "Processing watch 32/60 on page 2\n",
      "Processing watch 32/60 on page 2\n",
      "Processing watch 33/60 on page 2\n",
      "Processing watch 33/60 on page 2\n",
      "Processing watch 34/60 on page 2\n",
      "Processing watch 34/60 on page 2\n",
      "Processing watch 35/60 on page 2\n",
      "Processing watch 35/60 on page 2\n",
      "Processing watch 36/60 on page 2\n",
      "Processing watch 36/60 on page 2\n",
      "Processing watch 37/60 on page 2\n",
      "Processing watch 37/60 on page 2\n",
      "Processing watch 38/60 on page 2\n",
      "Processing watch 38/60 on page 2\n",
      "Processing watch 39/60 on page 2\n",
      "Processing watch 39/60 on page 2\n",
      "Processing watch 40/60 on page 2\n",
      "Processing watch 40/60 on page 2\n",
      "Processing watch 41/60 on page 2\n",
      "Processing watch 41/60 on page 2\n",
      "Processing watch 42/60 on page 2\n",
      "Processing watch 42/60 on page 2\n",
      "Processing watch 43/60 on page 2\n",
      "Processing watch 43/60 on page 2\n",
      "Processing watch 44/60 on page 2\n",
      "Processing watch 44/60 on page 2\n",
      "Processing watch 45/60 on page 2\n",
      "Processing watch 45/60 on page 2\n",
      "Processing watch 46/60 on page 2\n",
      "Processing watch 46/60 on page 2\n",
      "Processing watch 47/60 on page 2\n",
      "Processing watch 47/60 on page 2\n",
      "Processing watch 48/60 on page 2\n",
      "Processing watch 48/60 on page 2\n",
      "Processing watch 49/60 on page 2\n",
      "Processing watch 49/60 on page 2\n",
      "Processing watch 50/60 on page 2\n",
      "Processing watch 50/60 on page 2\n",
      "Processing watch 51/60 on page 2\n",
      "Processing watch 51/60 on page 2\n",
      "Processing watch 52/60 on page 2\n",
      "Processing watch 52/60 on page 2\n",
      "Processing watch 53/60 on page 2\n",
      "Processing watch 53/60 on page 2\n",
      "Processing watch 54/60 on page 2\n",
      "Processing watch 54/60 on page 2\n",
      "Processing watch 55/60 on page 2\n",
      "Processing watch 55/60 on page 2\n",
      "Processing watch 56/60 on page 2\n",
      "Processing watch 56/60 on page 2\n",
      "Processing watch 57/60 on page 2\n",
      "Processing watch 57/60 on page 2\n",
      "Processing watch 58/60 on page 2\n",
      "Processing watch 58/60 on page 2\n",
      "Processing watch 59/60 on page 2\n",
      "Processing watch 59/60 on page 2\n",
      "Processing watch 60/60 on page 2\n",
      "Processing watch 60/60 on page 2\n",
      "Completed page 2. Total watches so far: 120\n",
      "Completed page 2. Total watches so far: 120\n",
      "Loading page 3: https://www.chrono24.com/rolex/index-3.htm\n",
      "Loading page 3: https://www.chrono24.com/rolex/index-3.htm\n",
      "Found 60 watch listings on page 3\n",
      "Processing watch 1/60 on page 3\n",
      "Found 60 watch listings on page 3\n",
      "Processing watch 1/60 on page 3\n",
      "Processing watch 2/60 on page 3\n",
      "Processing watch 2/60 on page 3\n",
      "Processing watch 3/60 on page 3\n",
      "Processing watch 3/60 on page 3\n",
      "Processing watch 4/60 on page 3\n",
      "Processing watch 4/60 on page 3\n",
      "Processing watch 5/60 on page 3\n",
      "Processing watch 5/60 on page 3\n",
      "Processing watch 6/60 on page 3\n",
      "Processing watch 6/60 on page 3\n",
      "Processing watch 7/60 on page 3\n",
      "Processing watch 7/60 on page 3\n",
      "Processing watch 8/60 on page 3\n",
      "Processing watch 8/60 on page 3\n",
      "Processing watch 9/60 on page 3\n",
      "Processing watch 9/60 on page 3\n",
      "Processing watch 10/60 on page 3\n",
      "Processing watch 10/60 on page 3\n",
      "Processing watch 11/60 on page 3\n",
      "Processing watch 11/60 on page 3\n",
      "Processing watch 12/60 on page 3\n",
      "Processing watch 12/60 on page 3\n",
      "Processing watch 13/60 on page 3\n",
      "Processing watch 13/60 on page 3\n",
      "Processing watch 14/60 on page 3\n",
      "Processing watch 14/60 on page 3\n",
      "Processing watch 15/60 on page 3\n",
      "Processing watch 15/60 on page 3\n",
      "Processing watch 16/60 on page 3\n",
      "Processing watch 16/60 on page 3\n",
      "Processing watch 17/60 on page 3\n",
      "Processing watch 17/60 on page 3\n",
      "Processing watch 18/60 on page 3\n",
      "Processing watch 18/60 on page 3\n",
      "Processing watch 19/60 on page 3\n",
      "Processing watch 19/60 on page 3\n",
      "Processing watch 20/60 on page 3\n",
      "Processing watch 20/60 on page 3\n",
      "Processing watch 21/60 on page 3\n",
      "Processing watch 21/60 on page 3\n",
      "Processing watch 22/60 on page 3\n",
      "Processing watch 22/60 on page 3\n",
      "Processing watch 23/60 on page 3\n",
      "Processing watch 23/60 on page 3\n",
      "Processing watch 24/60 on page 3\n",
      "Processing watch 24/60 on page 3\n",
      "Processing watch 25/60 on page 3\n",
      "Processing watch 25/60 on page 3\n",
      "Processing watch 26/60 on page 3\n",
      "Processing watch 26/60 on page 3\n",
      "Processing watch 27/60 on page 3\n",
      "Processing watch 27/60 on page 3\n",
      "Processing watch 28/60 on page 3\n",
      "Processing watch 28/60 on page 3\n",
      "Processing watch 29/60 on page 3\n",
      "Processing watch 29/60 on page 3\n",
      "Processing watch 30/60 on page 3\n",
      "Processing watch 30/60 on page 3\n",
      "Processing watch 31/60 on page 3\n",
      "Processing watch 31/60 on page 3\n",
      "Processing watch 32/60 on page 3\n",
      "Processing watch 32/60 on page 3\n",
      "Processing watch 33/60 on page 3\n",
      "Processing watch 33/60 on page 3\n",
      "Processing watch 34/60 on page 3\n",
      "Processing watch 34/60 on page 3\n",
      "Processing watch 35/60 on page 3\n",
      "Processing watch 35/60 on page 3\n",
      "Processing watch 36/60 on page 3\n",
      "Processing watch 36/60 on page 3\n",
      "Processing watch 37/60 on page 3\n",
      "Processing watch 37/60 on page 3\n",
      "Processing watch 38/60 on page 3\n",
      "Processing watch 38/60 on page 3\n",
      "Processing watch 39/60 on page 3\n",
      "Processing watch 39/60 on page 3\n",
      "Processing watch 40/60 on page 3\n",
      "Processing watch 40/60 on page 3\n",
      "Processing watch 41/60 on page 3\n",
      "Processing watch 41/60 on page 3\n",
      "Processing watch 42/60 on page 3\n",
      "Processing watch 42/60 on page 3\n",
      "Processing watch 43/60 on page 3\n",
      "Processing watch 43/60 on page 3\n",
      "Processing watch 44/60 on page 3\n",
      "Processing watch 44/60 on page 3\n",
      "Processing watch 45/60 on page 3\n",
      "Processing watch 45/60 on page 3\n",
      "Processing watch 46/60 on page 3\n",
      "Processing watch 46/60 on page 3\n",
      "Processing watch 47/60 on page 3\n",
      "Processing watch 47/60 on page 3\n",
      "Processing watch 48/60 on page 3\n",
      "Processing watch 48/60 on page 3\n",
      "Processing watch 49/60 on page 3\n",
      "Processing watch 49/60 on page 3\n",
      "Processing watch 50/60 on page 3\n",
      "Processing watch 50/60 on page 3\n",
      "Processing watch 51/60 on page 3\n",
      "Processing watch 51/60 on page 3\n",
      "Processing watch 52/60 on page 3\n",
      "Processing watch 52/60 on page 3\n",
      "Processing watch 53/60 on page 3\n",
      "Processing watch 53/60 on page 3\n",
      "Processing watch 54/60 on page 3\n",
      "Processing watch 54/60 on page 3\n",
      "Processing watch 55/60 on page 3\n",
      "Processing watch 55/60 on page 3\n",
      "Processing watch 56/60 on page 3\n",
      "Processing watch 56/60 on page 3\n",
      "Processing watch 57/60 on page 3\n",
      "Processing watch 57/60 on page 3\n",
      "Processing watch 58/60 on page 3\n",
      "Processing watch 58/60 on page 3\n",
      "Processing watch 59/60 on page 3\n",
      "Processing watch 59/60 on page 3\n",
      "Processing watch 60/60 on page 3\n",
      "Processing watch 60/60 on page 3\n",
      "Completed page 3. Total watches so far: 180\n",
      "Completed page 3. Total watches so far: 180\n",
      "Reached maximum pages limit: 3\n",
      "\n",
      "Scraping Summary:\n",
      "Pages scraped: 3\n",
      "Total watches: 180\n",
      "Average watches per page: 60.0\n",
      "JSON saved to: scraped_watches.json\n",
      "CSV saved to: scraped_watches.csv\n",
      "Reached maximum pages limit: 3\n",
      "\n",
      "Scraping Summary:\n",
      "Pages scraped: 3\n",
      "Total watches: 180\n",
      "Average watches per page: 60.0\n",
      "JSON saved to: scraped_watches.json\n",
      "CSV saved to: scraped_watches.csv\n"
     ]
    }
   ],
   "source": [
    "start_url = \"https://www.chrono24.com/rolex/index.htm\"\n",
    "max_pages = 3\n",
    "\n",
    "print(f\"Starting scraper for {max_pages} pages\")\n",
    "print(f\"Target URL: {start_url}\")\n",
    "\n",
    "try:\n",
    "    scraper.start_scraping(start_url, max_pages=max_pages)\n",
    "    \n",
    "    progress = scraper.get_progress_info()\n",
    "    print(f\"\\nScraping Summary:\")\n",
    "    print(f\"Pages scraped: {progress['pages_scraped']}\")\n",
    "    print(f\"Total watches: {progress['total_watches']}\")\n",
    "    print(f\"Average watches per page: {progress['avg_watches_per_page']:.1f}\")\n",
    "    \n",
    "    json_file = scraper.save_to_json(\"scraped_watches.json\")\n",
    "    csv_file = scraper.save_to_csv(\"scraped_watches.csv\")\n",
    "    \n",
    "    if json_file:\n",
    "        print(f\"JSON saved to: {json_file}\")\n",
    "    if csv_file:\n",
    "        print(f\"CSV saved to: {csv_file}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Scraping failed: {e}\")\n",
    "finally:\n",
    "    scraper.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227502cd",
   "metadata": {},
   "source": [
    "## Data Analysis and Visualization\n",
    "\n",
    "After scraping, you can analyze the collected data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a750cd73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total watches: 180\n",
      "Unique watch IDs: 180\n",
      "Price statistics:\n",
      "  Min price: $3,123\n",
      "  Max price: $132,186\n",
      "  Average price: $21,511\n",
      "  Median price: $14,295\n",
      "Most common case materials:\n",
      "  Steel: 190\n",
      "  Gold/Steel: 80\n",
      "  Rose gold: 26\n",
      "  White gold: 24\n",
      "  Yellow gold: 24\n",
      "Most common movements:\n",
      "  Automatic: 358\n",
      "  3235: 39\n",
      "  3135: 22\n",
      "  3255: 11\n",
      "  4130: 11\n",
      "Analysis completed\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "def analyze_scraped_data(scraped_items):\n",
    "    if not scraped_items:\n",
    "        print(\"No data to analyze\")\n",
    "        return\n",
    "    \n",
    "    data = []\n",
    "    for item in scraped_items:\n",
    "        row = {\n",
    "            'watch_id': item.watch_id,\n",
    "            'title': item.watch_title,\n",
    "            'price': item.watch_price,\n",
    "        }\n",
    "        \n",
    "        if item.watch_details:\n",
    "            for section, details in item.watch_details.items():\n",
    "                if isinstance(details, dict):\n",
    "                    for key, value in details.items():\n",
    "                        row[f\"{section}_{key}\"] = value\n",
    "        \n",
    "        data.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    print(f\"Total watches: {len(df)}\")\n",
    "    print(f\"Unique watch IDs: {df['watch_id'].nunique()}\")\n",
    "    \n",
    "    prices = []\n",
    "    for price in df['price'].dropna():\n",
    "        numbers = re.findall(r'[\\d,]+', str(price).replace(',', ''))\n",
    "        if numbers:\n",
    "            try:\n",
    "                prices.append(int(numbers[0]))\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    if prices:\n",
    "        print(f\"Price statistics:\")\n",
    "        print(f\"  Min price: ${min(prices):,}\")\n",
    "        print(f\"  Max price: ${max(prices):,}\")\n",
    "        print(f\"  Average price: ${sum(prices)/len(prices):,.0f}\")\n",
    "        print(f\"  Median price: ${sorted(prices)[len(prices)//2]:,}\")\n",
    "    \n",
    "    print(\"Most common case materials:\")\n",
    "    case_materials = df.filter(regex='.*[Cc]ase material.*').stack().value_counts().head()\n",
    "    for material, count in case_materials.items():\n",
    "        print(f\"  {material}: {count}\")\n",
    "    \n",
    "    print(\"Most common movements:\")\n",
    "    movements = df.filter(regex='.*[Mm]ovement.*').stack().value_counts().head()\n",
    "    for movement, count in movements.items():\n",
    "        print(f\"  {movement}: {count}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "if hasattr(scraper, 'scraped_items') and scraper.scraped_items:\n",
    "    df = analyze_scraped_data(scraper.scraped_items)\n",
    "    print(\"Analysis completed\")\n",
    "else:\n",
    "    print(\"No scraped data available. Run the scraper first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f476487",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "\n",
    "Additional helper functions for data manipulation and analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1dea84e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_scraped_data(filename: str):\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        print(f\"Loaded {len(data)} watches from {filename}\")\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {filename} not found\")\n",
    "        return []\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Invalid JSON in {filename}\")\n",
    "        return []\n",
    "\n",
    "def merge_scraped_data(*filenames):\n",
    "    all_data = []\n",
    "    for filename in filenames:\n",
    "        data = load_scraped_data(filename)\n",
    "        all_data.extend(data)\n",
    "    \n",
    "    print(f\"Merged {len(all_data)} total watches from {len(filenames)} files\")\n",
    "    return all_data\n",
    "\n",
    "def filter_watches_by_price(data, min_price=None, max_price=None):\n",
    "    filtered = []\n",
    "    for watch in data:\n",
    "        price_str = watch.get('watch_price', '')\n",
    "        numbers = re.findall(r'[\\d,]+', price_str.replace(',', ''))\n",
    "        if numbers:\n",
    "            try:\n",
    "                price = int(numbers[0])\n",
    "                if (min_price is None or price >= min_price) and \\\n",
    "                   (max_price is None or price <= max_price):\n",
    "                    filtered.append(watch)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    print(f\"Filtered to {len(filtered)} watches within price range\")\n",
    "    return filtered\n",
    "\n",
    "def search_watches(data, search_term):\n",
    "    search_term = search_term.lower()\n",
    "    results = []\n",
    "    \n",
    "    for watch in data:\n",
    "        if search_term in watch.get('watch_title', '').lower():\n",
    "            results.append(watch)\n",
    "            continue\n",
    "        \n",
    "        details = watch.get('watch_details', {})\n",
    "        for section, values in details.items():\n",
    "            if isinstance(values, dict):\n",
    "                for key, value in values.items():\n",
    "                    if search_term in value.lower():\n",
    "                        results.append(watch)\n",
    "                        break\n",
    "                if watch in results:\n",
    "                    break\n",
    "    \n",
    "    print(f\"Found {len(results)} watches matching '{search_term}'\")\n",
    "    return results\n",
    "\n",
    "def convert_json_to_csv(json_filename, csv_filename):\n",
    "    data = load_scraped_data(json_filename)\n",
    "    if data:\n",
    "        flat_data = []\n",
    "        for watch in data:\n",
    "            flat_dict = {\n",
    "                \"watch_id\": watch.get(\"watch_id\", \"\"),\n",
    "                \"watch_title\": watch.get(\"watch_title\", \"\"),\n",
    "                \"watch_price\": watch.get(\"watch_price\", \"\")\n",
    "            }\n",
    "            \n",
    "            watch_details = watch.get(\"watch_details\", {})\n",
    "            if watch_details:\n",
    "                for section, details in watch_details.items():\n",
    "                    if isinstance(details, dict):\n",
    "                        for key, value in details.items():\n",
    "                            column_name = f\"{section}_{key}\".replace(\" \", \"_\").replace(\"/\", \"_\")\n",
    "                            flat_dict[column_name] = value\n",
    "            \n",
    "            flat_data.append(flat_dict)\n",
    "        \n",
    "        df = pd.DataFrame(flat_data)\n",
    "        df.to_csv(csv_filename, index=False, encoding='utf-8')\n",
    "        print(f\"Converted {json_filename} to {csv_filename}\")\n",
    "        return csv_filename\n",
    "    return None\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
